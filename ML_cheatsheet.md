<details>
<summary> ### 机器学习实战第二版  </summary>
**有监督：** K近邻、线性回归、逻辑回归、支持向量机SVM、决策树和随机森林、神经网络  
**无监督：** 聚类、K均值、DBSCAN、分层聚类HCA、异常检测和新颖性检测、单类SVM、孤立森林、可视化和降维、主成分分析、核主成分分析、局部线性嵌入LLE、关联规则学习、Apriori、Eclat  
**半监督：** 深度信念网络（DBN）基于一种互相堆叠的无监督组件，这个组件叫作受限玻尔兹曼机（RBM）。受限玻尔兹曼机以无监督方式进行训练，然后使用有监督学习技术对整个系统进行微调  
**强化学习** 智能体  
**批量学习** 离线学习，更新数据的同时，也要重新训练  
**增量学习** 在线学习，但是不一定是实时。适应大数据量的训练任务。适应不断变化的数据的速度：学习率  
数据庞大时另一种方法：选择Map reduce技术，跨多个服务器拆分进行批处理学习  
>在线学习面临的一个重大挑战是，如果给系统输入不良数据，系统
的性能将会逐渐下降。现在某些实时系统的客户说不定已经注意到了这
个现象。不良数据的来源可能是机器上发生故障的传感器，或者是有人
对搜索引擎恶意刷屏以提高搜索结果排名等。为了降低这种风险，你需
要密切监控系统，一旦检测到性能下降，就及时中断学习（可能还需要
恢复到之前的工作状态）。当然，同时你还需要监控输入数据，并对异
常数据做出响应（例如，使用异常检测算法）。
> 
**机器学习面临的问题** 训练数据不足、训练数据不具有代表性（泛化能力）、训练数据包含异常和噪声、无关特征  
>过拟合：收集更多的训练数据、减少模型的参数（增加正则化超参数）、修复数据中的错误和消除异常值  
>欠拟合：提供更好的特征、增加模型的参数、减少正则化超参数
>
**机器学习流程**
明确任务、数据挖掘、数据分析、数据预处理、选择并训练模型、微调模型、展示解决方案、系统上线  
</details>

### 机器学习知识点
1、模型：想要学习的条件概率分布或者决策函数；具体来说就是模型结构，比如CNN、RNN、LSTM、transformer；  
策略：从假设空间中选出参数最优的模型的准则，比如损失函数最小；比如均方差、交叉熵  
算法：如何求解全局最优解，比如优化算法：SGF、Adam  
2、归纳偏置：关于目标函数的必要假设  
3、逻辑回归处理的是分类问题，线性回归处理的是回归问题  
logit函数：对数几率函数  
几率=p/(1-p)  
![image](https://github.com/zhangwenjingustb/LookForWork/assets/141011729/da9487cb-d925-448f-b765-ea361fa86322)  
![image](https://github.com/zhangwenjingustb/LookForWork/assets/141011729/5aceed6a-572e-4948-acc0-79d84496ede7)  
逻辑回归：对特征加权求和，输入给sigmoid函数，输出概率，决定二分类的结果  
4、支持向量机  
寻找一个超平面，使得不同类别的数据到超平面的间隔（几何间隔）最大。  
函数间隔：在超平面w*\x+b=0确定的情况下，|w\*x+b|能够表示点x到距离超平面的远近，而通过观察w\*x+b的符号与类标记y的符号是否一致可判断分类是否正确，可以用(y\*(w\*x+b))的正负性来判定或表示分类的正确性。  
函数间隔=(y\*(w\*x+b))  
几何间隔就是函数间隔除以||w||，而且函数间隔y*(wx+b) = y*f(x)实际上就是|f(x)|，只是人为定义的一个间隔度量，而几何间隔|f(x)|/||w||才是直观上的点到超平面的距离。  
![image](https://github.com/zhangwenjingustb/LookForWork/assets/141011729/a0f22922-4354-442a-a6f7-badbcabb2407) ![image](https://github.com/zhangwenjingustb/LookForWork/assets/141011729/6a504cff-1047-4c74-93dd-18ec934f9b1b)  
![image](https://github.com/zhangwenjingustb/LookForWork/assets/141011729/40bcb04e-30aa-4c71-a053-52ac73b6f6fd)  
5、循环神经网络  
隐藏层上一时刻的输出值通过权重矩阵影响当前时刻的输出值：将输入和隐藏状态输入到全连接层和激活函数，产生当前输出值，并且再将当前输出值赋值给给下一隐藏状态  
![image](https://github.com/zhangwenjingustb/LookForWork/assets/141011729/af4ed96c-e4a9-469a-a518-ec47ad94192c)

6、LSTM  
![image](https://github.com/zhangwenjingustb/LookForWork/assets/141011729/05271c2c-c209-4139-aa05-9ba5f57f6e86)  
7、门控单元  
![image](https://github.com/zhangwenjingustb/LookForWork/assets/141011729/03a33c54-75df-4044-bad1-a1600a33b2aa)  
8、transformer  


9、决策树  
决策树的分裂依据是基尼系数或熵，基尼系数倾向于从树枝中分裂出最常见的类别，计算速度快；熵倾向于生成更平衡的树。  
10、损失函数（目标函数、成本函数）  
损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。  
交叉熵，衡量两个概率分布的差异  
H(第i个元素真实值和预测值)=-(j=1到j=q求和)[y^i(下标j)]*log[y尖^i(下标j)]  
交叉熵损失函数是H对于每个元素的平均值
带尖的是预测概率分布
补充知识：对于样本i，构造向量y(上标i)，第y(上标i)个元素为1，其余为0，只有索引为类别离散数值的这个元素为1  
![image](https://github.com/zhangwenjingustb/LookForWork/assets/141011729/7d7b64bd-4d70-4935-b4fb-aa5862e40d51)  
第一个是sigmoid，第二个是softmax  
11、优化算法：降低损失函数值的方法  
BP（反向传播算法）由正向传播和反向传播构成。在正向传播过程中，输入信息通过输入层经隐含层，逐层处理并传向输出层。如果在输出层得不到期望的输出值，则取输出与期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯量，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。  
反向传播算法不负责更新参数  
12、梯度消失和梯度爆炸  
梯度消失和梯度爆炸问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。靠近输入端的层由于之前梯度的积累，可能很小或很大，很小就会导致参数更新很慢，难以收敛；很大就会导致参数权重变化很快，直至优化算法发散。  
用Relu代替sigmoid，用BN和残差连接，正则化  
13、基于梯度的优化算法  
Ada梯度算法：每个维度设置不同的学习率  
Adam梯度算法：  
14、其他的优化算法：牛顿法，对函数进行二阶近似；最速梯度下降：每一步都对函数的梯度向量寻找最优步长  
15、最小二乘：通过最小化误差的平方和寻找数据的最佳函数匹配  
16、激活函数：  
原因：不仅可以逼近神经网络输出的线性组合，还可以逼近任何函数。如果没有激活函数，那就是感知机。  
![激活函数优劣对比](https://github.com/zhangwenjingustb/LookForWork/assets/141011729/a75bef59-c9b6-444b-9b86-e8a309f5e967)  
17、正则化：对某一问题进行限制，作用是防止过拟合  
L0范数：向量中非零元素的个数  
L1范数：向量个元素的绝对值之和，寻找最优的稀疏项。为什么稀疏好？因为稀疏化有利于：特征选择、可解释性  
L2范数：（又叫岭回归、权值衰减），平方和开根号，防止过拟合  
Dropout：网络训练过程中，以概率p丢弃部分神经元，被丢弃的神经元的输出为0  
在Dropout每一轮训练过程中随机丢失神经元的操作相当于多个DNNs进行取平均，因此用于预测时具有vote的效果。
减少神经元之间复杂的共适应性。当隐藏层神经元被随机删除之后，使得全连接网络具有了一定的稀疏化，从而有效地减轻了不同特征的协同效应。也就是说，有些特征可能会依赖于固定关系的隐含节点的共同作用，而通过Dropout的话，就有效地组织了某些特征在其他特征存在下才有效果的情况，增加了神经网络的鲁棒性。  
BN：（Batch Normalization），防止梯度消失，把每层神经网络任意神经元输入值的分布强行拉回到均值为0方差为1的标准正态分布，避免因为激活函数导致的梯度消失问题
>大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如transfer learning/domain adaptation等。而covariate shift就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同。大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。
>
18、归一化、标准化、正则化  
归一化：缩放仅仅跟最大、最小值的差别有关。  
标准化：缩放和每个点都有关系，通过方差（variance）体现出来。与归一化对比，标准化中所有数据点都有贡献（通过均值和标准差造成影响）。  
19、二分类的评价指标：  
正例（Positives）：关注的识别目标就是正例。  
![image](https://github.com/zhangwenjingustb/LookForWork/assets/141011729/4612ce0e-e619-4e08-a953-984b8a9ea2e8)
精确率和召回率均值的两倍：F1分数  
ROC曲线、AUC（area under curve）是面积，AUC的取值范围在0.5和1之间。AUC越接近1.0，检测方法真实性越高  
20、目标检测的评价指标：  
IoU交并比，真实框与预测框的交并比（面积）IoU=A∩B/A∪B  
21、图像分类评价指标：  
TopK: 对一张图片，模型给出的识别概率中（即置信度分数），分数排名前K位中包含有正确目标（正确的正例），则认为正确。  

















